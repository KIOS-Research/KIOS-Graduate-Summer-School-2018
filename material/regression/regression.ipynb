{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KIOS Graduate Summer School 2018 on Intelligent Systems and Control\n",
    "\n",
    "# Regression exercise\n",
    "\n",
    "Welcome to this practical session!\n",
    "\n",
    "In this exercise you will be asked to predict the energy efficiency of buildings given various building parameters, such as, the surface area and height. You will become familiar with **pandas** (a powerful tool for data analysis) and **scikit-learn** (a popular machine learning library).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "\n",
    "You will be asked to predict the heating load and cooling load of buildings given various building parameters. Specifically, the following information will be given for each building.\n",
    "\n",
    "    X1 Relative Compactness\n",
    "    X2 Surface Area\n",
    "    X3 Wall Area\n",
    "    X4 Roof Area\n",
    "    X5 Overall Height\n",
    "    X6 Orientation\n",
    "    X7 Glazing Area\n",
    "    X8 Glazing Area Distribution\n",
    "\n",
    "    y1 Heating Load\n",
    "    y2 Cooling Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![outline](images/outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries\n",
    "\n",
    "Run the following to import all the necessary libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For visualisations\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the random seed. Do not change the seed in order to allow the reproducibility of the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide the directory of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_training = 'datasets/energy_efficiency_training.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load the dataset into a pandas dataframe. [(Hint)](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally a good practise to randomly shuffle the dataset to make sure that the training/validation sets are representative of the overall distribution of the data. ([Hint](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html); use the random seed defined earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = #TODO\n",
    "df_training.reset_index(drop=True, inplace=True) # re-order the indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now find the dimensionality (shape) of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_shape = #TODO\n",
    "print 'Shape of the training set: ', training_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first three lines of the training set to display a small sample. [(Hint)](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate descriptive statistics that summarize the central tendency, dispersion and shape of the dataset’s distribution. ([Hint](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset in more detail. Print a consise summary of the training set that includes, among others, the type of the columns and the number of non-null values. [(Hint)](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the column names of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = #TODO\n",
    "print 'Columns:\\n', columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to obtain the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_all = ['Y1', 'Y2']\n",
    "features = [f for f in columns if f not in targets_all]\n",
    "print 'Features: ', features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's currently focus on a single target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Y1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the feature columns from the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_X = #TODO\n",
    "df_training_y = #TODO\n",
    "\n",
    "print 'Shape of training set (features): ', df_training_X.shape\n",
    "print 'Shape of training set (target): ', df_training_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing refers to a sequence of transformations applied to data before feeding them to a machine learning algorithm, for example:\n",
    "* dealing with missing values\n",
    "* dealing with outliers\n",
    "* feature scaling\n",
    "* converting categorical features to dummy variables (one hot encoding)\n",
    "* transforming skewed data distributions\n",
    "\n",
    "This practical exercise will focus on one such transformation called *feature scaling* that causes the features to have roughly the same magnitude. Without this step some features may gain more importance or have a higher influence. Feature rescaling is particularly useful for methods that consider a distance-related metric (e.g. k-NN) or use gradient descent (e.g. neural networks).\n",
    "\n",
    "One way to perform feature scaling is *feature standardisation*. where a continuous feature $X$ with mean $\\mu$ and standard deviation $\\sigma$ will be transformed as follows:\n",
    "\n",
    "<center>$X \\leftarrow \\frac{X - \\mu}{\\sigma}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by calculating the means ([Hint](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html)) and standard deviations ([Hint](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.std.html)) of the *df_training_X* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = #TODO\n",
    "stds = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes as inputs a dataframe, its feature means and standard deviations, and performs feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_features(df, means, stds):\n",
    "    df_std = #TODO\n",
    "    \n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function you have just written to standardise the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_X_std = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to observe a sample of the standardised dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df_training_X_std.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cross-validation* is used to help you identify the best model for the problem. In this exercise we will use the *holdout* cross-validation. In another exercise you will learn about the *k-fold* cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating the validation set (i.e. the holdout set). You are given the ratio of the training set that will form the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the *train* and *validation* sets from the *training* set. Do the same for the *standardised training* set. ([Hint](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html); disable data shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X, df_valid_X = #TODO\n",
    "df_train_X_std, df_valid_X_std = #TODO\n",
    "df_train_y, df_valid_y = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-order the dataframes' indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [df_train_X, df_valid_X, df_train_y, df_valid_y, df_train_X_std, df_valid_X_std]\n",
    "for d in splits:\n",
    "    d.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to display the dimensionalities of the created dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Shape of:'\n",
    "print 'df_train_X: ', df_train_X.shape\n",
    "print 'df_train_y: ', df_train_y.shape\n",
    "print 'df_valid_X: ', df_valid_X.shape\n",
    "print 'df_valid_y: ', df_valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Importance of feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes as inputs a machine learning model, training set and test set. Complete the function that trains ('fits') the model on the training set, makes predictions on both the training and test sets, and returns the performance. In this exercise our performance metric is the [mean squared error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html). (Hint: you may find the examples from this [tutorial](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html) useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, df_train_X, df_train_y, df_test_X, df_test_y):\n",
    "    # Fit model\n",
    "    #TODO\n",
    "    \n",
    "    # Predictions and performance (MSE) on training set\n",
    "    train_y_pred = #TODO\n",
    "    mse_train = #TODO\n",
    "    \n",
    "    # Predictions and performance (MSE) on test set\n",
    "    test_y_pred = #TODO\n",
    "    mse_test = #TODO\n",
    "    \n",
    "    return mse_train, mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a neural network ([MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)). Set the maximum number of iterations to be 1000 and don't forget to fix the random seed. Leave the rest of the parameters to their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(max_iter=1000, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *train_model* function you wrote earlier to train the neural network you have just defined, and obtain the performance on the original validation set and the standardised one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without feature scaling\n",
    "_, nn_mse_valid = #TODO\n",
    "\n",
    "# with feature scaling\n",
    "_, nn_mse_valid_std = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to see how important feature scaling is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Mean squared error on validation set : ', nn_mse_valid\n",
    "print 'Mean squared error on standardised validation set: ', nn_mse_valid_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the validation set to identify the best machine learning model. You will get the chance to try out both linear and non-linear models, specifically, you will use linear regression, a decision tree and a neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a linear regression model. ([LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain its performance on the validation set. Take a moment to think whether you need to use the original or standardised training / validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mse_valid_lr = train_model(lr, df_train_X, df_train_y, df_valid_X, df_valid_y)\n",
    "print 'Mean squared error on validation set using linear regression: ', '%.2f' % mse_valid_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a decision tree. ([DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor); don't forget to fix the random seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain its performance on the validation set. Take a moment to think whether you need to use the original or standardised training / validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train_dt, mse_valid_dt = train_model(dt, df_train_X, df_train_y, df_valid_X, df_valid_y)\n",
    "print 'Mean squared error on validation set using decision tree: ', '%.2f' % mse_valid_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a considerable improvement over linear regression! This is attributed to the fact that a decision tree is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now examine various neural network models! A neural network has many hyper-parameters and cross-validation will help us tune these and select the best.\n",
    "\n",
    "A neural netowrk is very sensitive to these hyper-parameters. To demonstrate these you will try out many neural networks. We have provided the model *nn1* below, define your own models *nn2*, *nn3* and *nn4* using [MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = MLPRegressor(\n",
    "        max_iter=1000,\n",
    "        hidden_layer_sizes = (100,),\n",
    "        activation = 'logistic',\n",
    "        solver = 'sgd',\n",
    "        alpha = 0.0001,\n",
    "        batch_size = 'auto',\n",
    "        learning_rate_init = 0.001,\n",
    "        learning_rate = 'constant',\n",
    "        random_state = seed\n",
    "    )\n",
    "\n",
    "nn2 = MLPRegressor(\n",
    "        max_iter=#TODO,\n",
    "        hidden_layer_sizes = #TODO,\n",
    "        activation = #TODO,\n",
    "        solver = #TODO,\n",
    "        alpha = #TODO,\n",
    "        batch_size = #TODO,\n",
    "        learning_rate_init = #TODO,\n",
    "        learning_rate = #TODO,\n",
    "        random_state = seed\n",
    "    )\n",
    "\n",
    "nn3 = MLPRegressor(\n",
    "        max_iter=#TODO,\n",
    "        hidden_layer_sizes = #TODO,\n",
    "        activation = #TODO,\n",
    "        solver = #TODO,\n",
    "        alpha = #TODO,\n",
    "        batch_size = #TODO,\n",
    "        learning_rate_init = #TODO,\n",
    "        learning_rate = #TODO,\n",
    "        random_state = seed\n",
    "    )\n",
    "\n",
    "nn4 = MLPRegressor(\n",
    "        max_iter=#TODO,\n",
    "        hidden_layer_sizes = #TODO,\n",
    "        activation = #TODO,\n",
    "        solver = #TODO,\n",
    "        alpha = #TODO,\n",
    "        batch_size = #TODO,\n",
    "        learning_rate_init = #TODO,\n",
    "        learning_rate = #TODO,\n",
    "        random_state = seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide a neural network model after we performed hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn5 = MLPRegressor(\n",
    "        max_iter=1000,\n",
    "        hidden_layer_sizes = (100, 16, 16),\n",
    "        activation = 'relu',\n",
    "        solver = 'adam',\n",
    "        alpha = 0.001,\n",
    "        batch_size = 16,\n",
    "        learning_rate_init = 0.001,\n",
    "        learning_rate = 'constant',\n",
    "        random_state = seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's obtain the performance of each model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all neural network models\n",
    "nn_models = [nn1, nn2, nn3, nn4, nn5]\n",
    "\n",
    "# Loop over all models\n",
    "for i in range(len(nn_models)):\n",
    "    nn = nn_models[i]\n",
    "    \n",
    "    _, mse_valid_nn = train_model(nn, df_train_X_std, df_train_y, df_valid_X_std, df_valid_y)\n",
    "    print 'Mean squared error on validation set using neural network nn' + str(i + 1), ': ', '%.2f' % mse_valid_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, a neural network has many hyper-parameters and it's very sensitive to them. For instance, *nn1* has a similar performance to linear regression while the well-tuned *nn5* performs slightly better than the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we wish to find out if getting more training data would be beneficial. The learning curves will come in handy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *train_model* function you defined earlier to calculate the mean squared error of the neural network model *nn5* on the training and validation sets for various given training set sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = list(range(10,460,50)) + [460]\n",
    "\n",
    "lst_mse_train_nn = []\n",
    "lst_mse_valid_nn = []\n",
    "\n",
    "for s in sizes:\n",
    "    mse_train_nn, mse_valid_nn = train_model(nn5,\n",
    "                                             #TODO,\n",
    "                                             #TODO,\n",
    "                                             df_valid_X_std,\n",
    "                                             df_valid_y)\n",
    "    \n",
    "    lst_mse_train_nn.append(mse_train_nn)\n",
    "    lst_mse_valid_nn.append(mse_valid_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to generate the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0)\n",
    "plt.title('Neural Network')\n",
    "plt.plot(sizes, lst_mse_train_nn, label='mse train')\n",
    "plt.plot(sizes, lst_mse_valid_nn, label='mse valid')\n",
    "leg = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can you conclude about getting more data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final evaluation will be conducted on an independent test set completely unseen by the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to load the test set into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory of test set\n",
    "dir_test = 'datasets/energy_efficiency_test.csv'\n",
    "\n",
    "# load test set\n",
    "df_test = pd.read_csv(dir_test, index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_X = #TODO\n",
    "df_test_y = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the performnace of the decision tree *dt* and neural network *nn5* is very close, we will focus on the former because, as we will show shortly, it is more 'interpretable'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the predictions and performance (MSE) on the test set for *dt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred_dt = #TODO\n",
    "mse_test_dt = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to display the MSE on all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Mean squared error on training set using decision tree' + ': ', '%.2f' % mse_train_dt\n",
    "print 'Mean squared error on validation set using decision tree' + ': ', '%.2f' % mse_valid_dt\n",
    "print 'Mean squared error on test set using decision tree' + ': ', '%.2f' % mse_test_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's print out a sample of the predictions to see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample test set')\n",
    "print(list(df_test_y)[:10])\n",
    "\n",
    "print '\\n'\n",
    "\n",
    "print('Predictions')\n",
    "print(list(test_y_pred_dt)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that some of the features have a stronger predictive power than others. Feature selection offers many potential benefits; it can boost regression or classification performance and provide insight to the data by returning only the top predictors. It can further facilitate data visualisation, reduce storage requirements and execution runtime of learning algorithms.\n",
    "\n",
    "We will learn about <span style=\"color:red\">feature selection</span> and <span style=\"color:red\">dimensionality reduction</span> (feature projection) in detail in another practical exercise. For now, we provide a list of the 'best' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = ['X1', 'X7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to define a new decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on the selected subset of features and calculate the performance on all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train set\n",
    "#TODO\n",
    "\n",
    "# performance on train set\n",
    "train_y_pred = #TODO\n",
    "mse_train = metrics.mean_squared_error(df_train_y, train_y_pred)\n",
    "\n",
    "# performance on validation set\n",
    "valid_y_pred = #TODO\n",
    "mse_valid = metrics.mean_squared_error(df_valid_y, valid_y_pred)\n",
    "\n",
    "# performance on test set\n",
    "test_y_pred = #TODO\n",
    "mse_test = metrics.mean_squared_error(df_test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following to display the MSE on all datasets using feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Mean squared error on training set using decision tree: ', '%.2f' % mse_train\n",
    "print 'Mean squared error on validation set using decision tree: ', '%.2f' % mse_valid\n",
    "print 'Mean squared error on test set using decision tree: ', '%.2f' % mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have mentioned earlier that a decision tree is highly 'interpretable' compared to other models. Let's visualise it to see why! Check the file *tree.png* that will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(model, out_file='tree.dot', \n",
    "                         feature_names=best_features,  \n",
    "                         class_names=target,\n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing this practical exercise! :-)\n",
    "\n",
    "There are still plenty of things you could experiment with, such as:\n",
    "* Change target to 'Y2' to see the new behaviour and performance of the models (Section 2)\n",
    "* Try out more machine learning models such as SVMs (Section 6)\n",
    "* Experiment with other feature subsets (Section 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
